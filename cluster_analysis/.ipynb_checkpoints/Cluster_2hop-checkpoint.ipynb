{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository numbers: 2960\n",
      "https://github.com/natcap/natgeo-dams': \n",
      "['pypng', 'requests', 'alabaster', 'codecov', 'detox', 'docutils', 'flake8', 'httpbin', 'more-itertools', 'pysocks', 'pytest', 'pytest-cov', 'pytest-httpbin', 'pytest-mock', 'pytest-xdist', 'readme-renderer', 'sphinx', 'tox', 'apipkg', 'appdirs', 'atomicwrites', 'attrs', 'babel', 'bleach', 'blinker', 'brotlipy', 'certifi', 'cffi', 'chardet', 'click', 'configparser', 'contextlib2', 'coverage', 'decorator', 'distlib', 'dnspython', 'entrypoints', 'enum34', 'eventlet', 'execnet', 'filelock', 'flask', 'funcsigs', 'functools32', 'greenlet', 'idna', 'imagesize', 'importlib-metadata', 'importlib-resources', 'itsdangerous', 'jinja2', 'markupsafe', 'mccabe', 'mock', 'monotonic', 'pathlib2', 'pluggy', 'py', 'pycodestyle', 'pycparser', 'pyflakes', 'pygments', 'pytest-forked', 'pytz', 'raven', 'scandir', 'singledispatch', 'six', 'snowballstemmer', 'toml', 'typing', 'urllib3', 'virtualenv', 'webencodings', 'werkzeug', 'zipp']\n"
     ]
    }
   ],
   "source": [
    "input_file ='../data/results_3k_2hop.json'\n",
    "\n",
    "# with open(input_file) as f:\n",
    "#     repo_data = json.load(f,strict=False)\n",
    "#  JSONDecodeError: Extra data: line 2 column 1 (char 969)\n",
    "    \n",
    "file = open(input_file, 'r', encoding='utf-8')\n",
    "repo_data = {}\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    repo_data = {**repo_data,**dic}\n",
    "    \n",
    "print(f'Repository numbers: {len(repo_data)}')\n",
    "print(f'''https://github.com/natcap/natgeo-dams': \n",
    "{repo_data['https://github.com/natcap/natgeo-dams']}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  problem: why 2960?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "1. repository name -> index\n",
    "2. dependency name -> index\n",
    "3. build dep matrix  matrix shape (3012,15663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## id->name name->id dictionary\n",
    "rep_name2index = {}\n",
    "dep_name2index = {}\n",
    "for rep,deps in repo_data.items():\n",
    "    rep_name2index.setdefault(rep,len(rep_name2index))\n",
    "    for dep in deps:\n",
    "        dep_name2index.setdefault(dep,len(dep_name2index))\n",
    "\n",
    "rep_index2name = {}\n",
    "dep_index2name = {}\n",
    "for k,v in rep_name2index.items():\n",
    "    rep_index2name[v] = k\n",
    "\n",
    "for k,v in dep_name2index.items():\n",
    "    dep_index2name[v] = k\n",
    "\n",
    "rep_num = len(rep_name2index)\n",
    "dep_num = len(dep_name2index)\n",
    "    \n",
    "# build matrix\n",
    "repo_mat = np.zeros((rep_num,dep_num))  # (3012, 15663)\n",
    "for rep in repo_data:\n",
    "    rep_index = rep_name2index[rep] # row number\n",
    "    for dep in repo_data[rep]:\n",
    "        dep_index =dep_name2index[dep] # col number\n",
    "        repo_mat[rep_index][dep_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2960, 2960),\n",
       " array([[1.        , 0.09410436, 0.        , ..., 0.13905533, 0.14509525,\n",
       "         0.12681432],\n",
       "        [0.09410436, 1.        , 0.03566882, ..., 0.32928732, 0.22558942,\n",
       "         0.31905175],\n",
       "        [0.        , 0.03566882, 1.        , ..., 0.09325048, 0.15811388,\n",
       "         0.10050378],\n",
       "        ...,\n",
       "        [0.13905533, 0.32928732, 0.09325048, ..., 1.        , 0.58976782,\n",
       "         0.77787815],\n",
       "        [0.14509525, 0.22558942, 0.15811388, ..., 0.58976782, 1.        ,\n",
       "         0.38138504],\n",
       "        [0.12681432, 0.31905175, 0.10050378, ..., 0.77787815, 0.38138504,\n",
       "         1.        ]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  get the cosine similarity matrix\n",
    "cos_sim_mat = cosine_similarity(repo_mat)\n",
    "cos_sim_mat.shape, cos_sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2960, 28477), array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_mat.shape,repo_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Kmeans with repo_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters= 2 score1: 1037.3159999545803 score2: 0.6070374259660937 score3: 1.251053509803207\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clusters = list(range(2,15))\n",
    "c_h_scores,s_scores,d_scores = [],[],[]\n",
    "for k in clusters:\n",
    "    y_pred = KMeans(n_clusters=k, init='k-means++', max_iter=500).fit_predict(repo_mat)\n",
    "    c_h_score= metrics.calinski_harabasz_score(repo_mat, y_pred) # maximize \n",
    "    s_score = metrics.silhouette_score(repo_mat, y_pred)         # [-1,1]  want it close to 1\n",
    "    d_score = metrics.davies_bouldin_score(repo_mat, y_pred)     # minimize  want it close to 0\n",
    "    c_h_scores.append(c_h_score)\n",
    "    s_scores.append(s_score)\n",
    "    d_scores.append(d_score)\n",
    "    print(\"n_clusters=\", k,\"score1:\",c_h_score,\"score2:\",s_score,\"score3:\",d_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => result clusetr = 4 OR 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Kmeans + PCA with repo_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(0.95)  \n",
    "X_pca = pca.fit_transform(repo_mat)\n",
    "print(X_pca.shape)\n",
    "\n",
    "clusters = list(range(2,15))\n",
    "c_h_scores,s_scores,d_scores = [],[],[]\n",
    "for k in clusters:\n",
    "    y_pred = KMeans(n_clusters=k, init='k-means++', max_iter=500).fit_predict(X_pca)\n",
    "    c_h_score= metrics.calinski_harabasz_score(X_pca, y_pred) # maximize \n",
    "    s_score = metrics.silhouette_score(X_pca, y_pred)         # [-1,1]  want it close to 1\n",
    "    d_score = metrics.davies_bouldin_score(X_pca, y_pred)     # minimize  want it close to 0\n",
    "    c_h_scores.append(c_h_score)\n",
    "    s_scores.append(s_score)\n",
    "    d_scores.append(d_score)\n",
    "    print(\"n_clusters=\", k,\"score1:\",c_h_score,\"score2:\",s_score,\"score3:\",d_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => result clusetr = 3 OR 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Kmeans with cos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clusters = list(range(2,15))\n",
    "c_h_scores,s_scores,d_scores = [],[],[]\n",
    "for k in clusters:\n",
    "    y_pred = KMeans(n_clusters=k, init='k-means++', max_iter=500).fit_predict(cos_sim_mat)\n",
    "    c_h_score= metrics.calinski_harabasz_score(cos_sim_mat, y_pred) # maximize \n",
    "    s_score = metrics.silhouette_score(cos_sim_mat, y_pred)         # [-1,1]  want it close to 1\n",
    "    d_score = metrics.davies_bouldin_score(cos_sim_mat, y_pred)     # minimize  want it close to 0\n",
    "    c_h_scores.append(c_h_score)\n",
    "    s_scores.append(s_score)\n",
    "    d_scores.append(d_score)\n",
    "    print(\"n_clusters=\", k,\"score1:\",c_h_score,\"score2:\",s_score,\"score3:\",d_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => result clusetr = 3 OR 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Cluster \n",
    "\n",
    "The edge weight value between two points farther apart is lower, while the edge weight value between two points closer together is higher.\n",
    "\n",
    "Strength:<br>\n",
    "1) Spectral clustering only needs the similarity matrix between data, so it is very effective for processing sparse data clustering. This is difficult for traditional clustering algorithms such as K-Means.<br>\n",
    "2) Due to the use of dimensionality reduction, the complexity of processing high-dimensional data clustering is better than traditional clustering algorithms.\n",
    "\n",
    "Weakness:<br>\n",
    "1) If the dimensionality of the final cluster is very high, the running speed of spectral clustering and the final clustering effect are not good due to insufficient dimensionality reduction.<br>\n",
    "2) The clustering effect depends on the similarity matrix, and the final clustering effect obtained by different similarity matrices may be very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clusters = list(range(2,15))\n",
    "c_h_scores,s_scores,d_scores = [],[],[]\n",
    "for k in clusters:\n",
    "    y_pred = SpectralClustering(n_clusters=k,affinity='precomputed').fit_predict(cos_sim_mat)\n",
    "    c_h_score= metrics.calinski_harabasz_score(cos_sim_mat, y_pred) # maximize \n",
    "    s_score = metrics.silhouette_score(cos_sim_mat, y_pred)         # [-1,1]  want it close to 1\n",
    "    d_score = metrics.davies_bouldin_score(cos_sim_mat, y_pred)     # minimize  want it close to 0\n",
    "    c_h_scores.append(c_h_score)\n",
    "    s_scores.append(s_score)\n",
    "    d_scores.append(d_score)\n",
    "    print(\"n_clusters=\", k,\"score1:\",c_h_score,\"score2:\",s_score,\"score3:\",d_score)\n",
    "    \n",
    "#Warning:\n",
    "#/opt/anaconda3/envs/Knowledge_Graph/lib/python3.6/site-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
    "#  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => result clusetr = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN Cluster \n",
    "\n",
    "strength:\n",
    "1. Compared with K-Means, HDBSCAN does not need to declare the number of clusters in advance.\n",
    "2. It can cluster dense data sets of any shape. In contrast, clustering algorithms such as K-Means are generally only suitable for convex data sets.\n",
    "3. Abnormal points can be found while clustering, and it is not sensitive to abnormal points in the data set.\n",
    "4. The clustering results are not biased. In contrast, the initial values of clustering algorithms such as K-Means have a great influence on the clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "#params \n",
    "min_cluster_sizes = [5,10,15,20]\n",
    "min_sampless = [10,20,30,40,50]\n",
    "cluster_selection_epsilons = [0.1,0.3,0.5]\n",
    "alphas = [0.25,0.5,0.75,1.0]\n",
    "\n",
    "\n",
    "for min_cluster_size in min_cluster_sizes:\n",
    "    for min_samples in min_sampless:\n",
    "        for cluster_selection_epsilon in cluster_selection_epsilons:\n",
    "            for alpha in alphas:\n",
    "                print(\"min_cluster_size:\",min_cluster_size,'min_samples:',min_samples,\n",
    "                      \"cluster_selection_epsilon:\",cluster_selection_epsilon,'alpha:',alpha)\n",
    "\n",
    "                cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=min_samples,\n",
    "                                          cluster_selection_epsilon=cluster_selection_epsilon,alpha=alpha)\n",
    "                y_pred= cluster.fit_predict(cos_sim_mat)\n",
    "                labels = cluster.labels_\n",
    "\n",
    "                c_h_score= metrics.calinski_harabasz_score(cos_sim_mat, y_pred) # maximize \n",
    "                s_score = metrics.silhouette_score(cos_sim_mat, y_pred)         # [-1,1]  want it close 1\n",
    "                d_score = metrics.davies_bouldin_score(cos_sim_mat, y_pred)     # minimize \n",
    "                y_unique = np.unique(labels)\n",
    "                n_clusters = y_unique.size - (1 if -1 in y_unique else 0)\n",
    "                print(\"clusters:\",n_clusters ,\"score1:\",c_h_score,\"score2:\",s_score,\"score3:\",d_score)\n",
    "                print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_cluster_size: 10 min_samples: 40 cluster_selection_epsilon: 0.3 alpha: 1.0\n",
    "clusters: 7 score1: 461.83448416490046 score2: 0.10818213466051357 score3: 1.29953322175402\n",
    "                \n",
    "min_cluster_size: 20 min_samples: 50 cluster_selection_epsilon: 0.3 alpha: 1.0\n",
    "clusters: 5 score1: 516.2251302409094 score2: 0.07802924382561442 score3: 1.583326675965355\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
